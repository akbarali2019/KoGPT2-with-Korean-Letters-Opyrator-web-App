# KoGPT2-with-Korean Letters-Opyrator-web-App
KoGPT2 with Opyrator web App Practice with 한글 글자 in a localhost

GPT-2는 주어진 텍스트의 다음 단어를 잘 예측할 수 있도록 학습된 언어모델이며 문장 생성에 최적화 되어 있습니다. KoGPT2는 부족한 한국어 성능을 극복하기 위해 40GB 이상의 텍스트로 학습된 한국어 디코더(decoder) 언어모델입니다.

# Dictionary
사전 크기는 51,200 이다.

# Data
한국어 위키 백과 이외, 뉴스, 모두의 말뭉치 v1.0, 청와대 국민청원 등의 다양한 데이터가 모델 학습에 사용되었습니다.
#
#
![image](https://user-images.githubusercontent.com/52565814/179814198-1dfb4f8e-cbe1-46e2-a51b-bff1badf15e4.png)
#
#
![image](https://user-images.githubusercontent.com/52565814/179793174-0de59e9d-825a-4e2d-b9d4-c40e92295db7.png)


